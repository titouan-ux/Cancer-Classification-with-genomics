---
title: "TP II IFSBM UE 12: Big Data and predictive models"
subtitle: "Introduction to deep learning."
author:
  - Loic Verlingue, Md, PhDc loic.verlingue@gustaveroussy.fr
  - Yoann Pradat, PhDc yoann.pradat@centralesupelec.fr
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    code_folding: show
  rmdformats::material:
    highlight: kate
---

<style>
body {
text-align: justify}
</style>

```{r ifsbm logo, echo = FALSE, out.width = '50%', fig.align="center"}
knitr::include_graphics('../../img/ifsbm_logo.jpeg')
```

# 1. What you will do in this R Notebook

## 1. Introduction

This is the second day of your training in R for Machine Learning. You will now enter the wide, wild and exciting field
of Deep Learning! Neural Networks and Deep Learning have revolutionized the performance of many Machine Learning tasks 
in image analysis, video analysis, natural language processing and many other "traditional" classification, regression
or clustering tasks.

As for the first lab, you will be dealing with a huge amount of variables (genes expression per patient) compared to the
number of patients. You again would like to **provide the clinician with a simple output** that he can use for his patients.

## 1.2 Main steps

In this notebook, you will be guided through TCGA data and images from the MNIST data. From expression data we will try 
to implement a neural network to perform classification tasks while from images we will also implement a (convolutional)
neural network to also perform classification.

Neural networks and machine learning are at the heart of current innovations! To go further, join us for exciting
research projects! 

## 1.3 In practice

The code is provided to you. You will just have to follow the instructions all along the notebook.
Explanations and corrections will be given throughout the notebook for a subset of the questions.

**IMPORTANT**
There are questions in this notebook that you need to answer and code that you need to write by yourself during the lab
or at home. The questions are indicated by a tag "QUESTION" while the code you need to write is indicated by a tag
"INCLASS WORK".

# 2. Build your first neural network with Keras.

## 2.1 About Keras.

[Keras](https://keras.io/) is a Python deep learning library designed to help human build and train neural networks of 
many different types. It builds on top of [Tensorflow 2](https://www.tensorflow.org/), an end-to-end open source
platform for machine leanring. The Keras has an R interface through the [Keras R package](https://keras.rstudio.com/)
which will be using in this lab.

The math is pre-computed, and functions are relatively user-friendly: is a high level API. It can use several
lower level back-ends for the math and calculation. Keras and Tensorflow have been develloped by Google
and are open source.

In order to install Keras using the R package, you will need Anaconda and a Python environment with tensorflow 2
installed. If `Anaconda` (or Miniconda) is not installed on your computer, download and install it. Then, run the
following chunk that will create a conda environment named `r_tensorflow` with the libraries required to run Keras.

```{r load keras library}
library(keras)
use_condaenv(condaenv="ifsbm_tp2")
```

You can check with the command `conda info --envs` run from a command line interface that an environment named
`ifsbm_tp2` exists. If not, read the instructions from the README. After running this cell, restart your R studio. Check
that your installation is correct by verifying that the next chunk does not return an error.

```{r test keras installation}
model <- keras_model_sequential()
```

For the next steps we will be using some `R` libraries. Load them with the following

```{r load libraries, message=FALSE}
library(RColorBrewer)
library(dplyr)
library(knitr)
library(tidyr)
library(yarrr)
```

We will also use some useful functions.

```{r useful functions}
getColors <- function(vec, pal="Dark2", alpha=0.7){
  colors <- list()

  palette_predefined <- read.csv("../data/colors.csv")
  palette_predefined <- setNames(palette_predefined$Color, palette_predefined$Name)
  palette_default <- brewer.pal(max(length(unique(vec)),3), pal)
  i <- 1
  for (v in unique(vec)){
    if (toupper(v) %in% names(palette_predefined)){
      colors[[as.character(v)]] <- adjustcolor(palette_predefined[[toupper(v)]], alpha)
    } else {
      colors[[as.character(v)]] <- adjustcolor(palette_default[[i]], alpha)
      i <- i+1
    }
  }
  colors
}

getConfusionMatrix <- function(labelsPredicted, labelsCorrect, labels=NULL){
  if (is.null(labels)){
    labels <- sort(unique(union(labelsPredicted, labelsCorrect)))
  } else {
    labels <- sort(unique(labels))
  }
  confMat <- data.frame(row.names=labels)

  for (labelPredicted in labels){
    for (labelCorrect in labels){
      confMat[labelPredicted, labelCorrect] <- sum(labelsPredicted==labelPredicted & labelsCorrect==labelCorrect)
    }
  }

  confMat
}
```

## 2.2 Load the data

As in the previous lab, we will be using data from the TCGA project. More specifically we will continue to load
expression data restricted to genes from the Cancer Gene Census.

```{r load cbioportal function}
source("../../lib/LoadcBioportal.R")
```

```{r load tcga expression data}
CgcTable <- read.csv("../data/CancerGeneCensusCOSMIC_20230117.csv")
CgcGenes <- CgcTable$Gene.Symbol

TcgaData <- LoadcBioportal(Organ="luad_tcga_pan_can_atlas_2018|lusc_tcga_pan_can_atlas_2018",
                           Genes=CgcGenes,
                           ClinicNeeded=T,
                           MutNeeded=F,
                           MutExtNeeded=F,
                           RNANeeded=T,
                           NormalizeRNA=T)

print(table(TcgaData$CLINIC$study))
colorsStudy <- getColors(TcgaData$CLINIC$study, alpha=0.7)
TcgaData$CLINIC$study_num <- ifelse(TcgaData$CLINIC$study=="luad", 0, 1)
```

## 2.3. Visualize your data

As already explained in the previous lab, it is an important step than visualizing your data and performing some vanilla
analyses. Let us reproduce the same plots as for the `blca|lusc` comparison.

**The distribution plot**

```{r visualize expression data per gene, fig.cap="Genes expressions", fig.height=8, fig.width=8, fig.align="center"}
set.seed(1995)
colorsStudy <- getColors(TcgaData$CLINIC$study, alpha=1)
genesExp <- colnames(TcgaData$EXP)
genesHist <- sort(sample(genesExp, size=9, replace=F))
par(mfrow=c(3,3), mai=c(0.35,0.4,0.35,0.4))

for (geneHist in genesHist){
  mask_luad <- TcgaData$CLINIC$study=="luad"
  mask_lusc <- TcgaData$CLINIC$study=="lusc"
  DataGene <- rbind(data.frame(Expression=TcgaData$EXP[mask_luad,geneHist], Tumor="luad"),
                    data.frame(Expression=TcgaData$EXP[mask_lusc,geneHist], Tumor="lusc"))

  pirateplot(formula=Expression ~ Tumor,
             data=DataGene,
             theme=1,
             quant=NULL,
             pal=colorsStudy,
             main=geneHist,
             xlab="")
}
```

**The heatmap plot**

```{r visualize expression data heatmap, fig.cap="TCGA expression data LUAD and LUSC", fig.height=8, fig.width=8, fig.align="center"}
rowNames <- rownames(TcgaData$EXP)
rowStudy <- TcgaData$CLINIC[rowNames, "study"]
rowSideColors <- sapply(rowStudy, function(x) colorsStudy[[x]])
heatmap(as.matrix(TcgaData$EXP), Rowv=NULL, Colv=NULL, keep.dendro=F, RowSideColors=rowSideColors)
```

**The PCA plot**

```{r compute PCA}
X <- t(t(TcgaData$EXP) - rowMeans(t(TcgaData$EXP)))
resSVD <- svd(X, nu=2, nv=2)
# the scores are the principal components i.e the 
# low-dimensional representations of the samples
scores <- resSVD$u %*% diag(resSVD$d[1:2])
```

```{r plot PCA,fig.cap="PCA TCGA expression data LUAD and LUSC", fig.height=6, fig.width=6, fig.align="center"}
# plot the two first principal components
plot(scores[,1],scores[,2],
     col=unlist(colorsStudy[TcgaData$CLINIC$study]), 
     pch=16,main='PCA representation',
     xlab=paste("dim1 = ",100*round(resSVD$d[1]^2/sum(resSVD$d^2),3),"%",sep = ""),
     ylab=paste("dim2 = ",100*round(resSVD$d[2]^2/sum(resSVD$d^2),3),"%",sep = ""))
legend("bottomright",legend=c("luad","lusc"), pch=16, cex=0.8, col=c(colorsStudy[["luad"]], colorsStudy[["lusc"]]))
```

## 2.4 Build a neural network

### Train/test split

For the purpose of assessing correctly the generalization capabilities of our future model we split the data into a
train and a test part.

```{r split train/test}
studySize <- nrow(TcgaData$CLINIC)
trainProp <- 0.8
trainIndex <- sample(seq(studySize), size=round(studySize*trainProp))
testIndex <-seq(studySize)[!seq(studySize) %in% trainIndex]

print(paste("Training size:", length(trainIndex)))
print(paste("Test size:", length(testIndex)))
```

### Model building

Building the neural network means building individual layers and connecting them through different methods. Once your
have build your network, you can compile and then fit it on the data.

The basic building block of a neural network is the **layer**. Layers sequentially learn representations of the data fed
into them and outputs their new representation to the following layer until the last layer tha provides a representation
suitable for performing a given tasks.

**INCLASS WORK**  Define a neural network by defining a sequence of dense layers. For this first exercice, you have
build a network with only one hidden layer. The input layer should take as input a vector of size the number of genes
in the `TcgaData$EXP` table while the output layer should produce a 2D vector of "class conditional probabilities"
(neural networks outputs are not in general probabilities). 

*Hint*: In order to build your first model you will need the following functions
1. `keras_model_sequential` to initialize the model object.
2. `layer_dense` to create dense layers (i.e fully connected).

You may find information in the [online documentation](https://cran.r-project.org/web/packages/keras/keras.pdf) of the R
keras package.

```{r first model}
input_shape <- ncol(TcgaData$EXP)

model <- keras_model_sequential()
model %>%
 layer_dense(input_shape = input_shape, units = 128, activation = 'relu') %>%
 layer_dense(units = 30, activation = 'relu') %>%
 layer_dense(units = 1, activation = 'sigmoid'
             )
# 1 hidden couche et 1 couche de sortie
```

You can visualize your model architecture by just printing the model as in the following chunk

```{r first model summary, eval=T}
print(model)
```

**QUESTION 1)** What does the `activation` parameter represent in the `layer_dense` function from keras? Give example
values for this parameter.

**ANSWER**:The activation parameter is used to specify the activation function to be applied to the output of the dense layer.
The goal of activation function is to introduce non-linearity into a neural network, allowing it to learn more complex relationships between inputs and outputs. The activation function is applied element-wise to the output of the dense layer before it is passed on to the next layer.
The classical activation function are RELU, Softmax, Tanh, sigmoïd...

&nbsp;

### Compile the model

Before the model is ready for training, it needs a few more settings. These are added during the model's compilation 
step. In particular, you need to specify the following

1. **The loss function** - This measures how accurate the model is during training. We want to minimize this function to
   "steer" the model in the right direction. Careful designing of the loss function allows to train networks with
   specific properties (regularized for instance).

2. **Optimizer** - This is the numerical methods used to update the network's weights so as to minimize the loss
   function. 

3. **Metrics** - Used to monitor the training and testing steps. The following example uses accuracy, the fraction of
   the samples that are correctly classified.


**INCLASS WORK**  Apply the compilation step on your model by specifying the parameters mentioned above. You should use
`adam` as an optimizer, `binary_crossentropy` as a loss function and `accuracy` as a metric.

*Hint*: Look into [this example](https://tensorflow.rstudio.com/guide/keras/) and into the documentation of the
`compile` function.

```{r compile first model}

model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
  )


#### YOUR WORK HERE
```

### Train your network

In order to train your neural network, you will have to provide it with the training data (predictive variables and
predicted variable) as well as extra parameters specifiying the `validation_split` for assessing the model performance
during the training, the number of `epochs` or the batch size.

**QUESTION 2)** What are epochs? What is the batch size? What could be the advantage of reducing or increasing the batch
size?

**ANSWER**:
Epochs is the number of times the data will be run. That is to say that we will go over each batch several times. 
The size of the batch is the size of the grouping of data by batch. Increasing the batch size allows us to have a faster training model but reducing it allows us to have more training because more data is processed.

&nbsp;

A very nice way to visualize the evolution of the network accuracy is through the learning curves which may take
multiple forms. In particular, one type of learning curve is the plotting of the score of your model as it moves
through the epochs. You can do this plot very simply by applying the `plot` function on the `history` object produced by
the `fit` function from the previous chunk.

**INCLASS WORK:** Use the `fit` method to train your neural network on the training data and draw the learning curves.
Train it on 50 epochs using a validation split of 20%.


```{r train first model}
model %>% fit(as.matrix(TcgaData$EXP[trainIndex,]), TcgaData$CLINIC$study_num[trainIndex], epochs = 50,validation_split=0.2)

#### YOUR WORK HERE
```

## Evaluate accuracy
Next, compare how the model performs on the test dataset:

**INCLASS WORK** Use the `evaluate` method to evaluate the quality of your neural network on test data. How does it
compare to the train accuracy? The validation accuracy? Use the `predict_classes` method to compute the confusion
matrices on the train and test sets.

```{r evaluate first model}
model %>% evaluate(as.matrix(TcgaData$EXP[testIndex,]), TcgaData$CLINIC$study_num[testIndex], verbose = 2)

#### YOUR WORK HERE
```

**INCLASS WORK** Experiment with other architectures by adding more layers, changing the layer sizes and/or adding
regularization methods like `dropout` or `batch normalization`. Read about these online or in the documentation.

```{r other models}
new_model <- keras_model_sequential()
new_model %>%
 layer_dense(input_shape = input_shape, units = 128, activation = 'relu') %>%
 layer_dropout(rate = 0.2) %>%
 layer_batch_normalization() %>%
 layer_dense(units = 128, activation = 'relu',) %>%
 layer_dropout(rate = 0.2) %>%
 layer_batch_normalization() %>%
 layer_dense(units = 1, activation = 'sigmoid'
             )

new_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
  )
new_model %>% fit(as.matrix(TcgaData$EXP[trainIndex,]), TcgaData$CLINIC$study_num[trainIndex], epochs = 50,validation_split=0.2)

```
```{r}
new_model %>% evaluate(as.matrix(TcgaData$EXP[testIndex,]), TcgaData$CLINIC$study_num[testIndex], verbose = 2)


```

Let us now look at the performance of the model on the MNIST dataset.

```{r evaluate other models, eval=T}
studyPredictedTrain <- new_model %>% predict(x=as.matrix(TcgaData$EXP[trainIndex,])) %>% `>` (0.5) %>% k_cast("int32")
studyPredictedTest <- new_model %>% predict(x=as.matrix(TcgaData$EXP[testIndex,])) %>% `>` (0.5) %>% k_cast("int32")

studyPredictedTrain <- ifelse(studyPredictedTrain==0, "luad", "lusc")
studyPredictedTest <- ifelse(studyPredictedTest==0, "luad", "lusc")

confMatTrain <- getConfusionMatrix(studyPredictedTrain, TcgaData$CLINIC$study[trainIndex])
confMatTest <- getConfusionMatrix(studyPredictedTest, TcgaData$CLINIC$study[testIndex])

ktrain <- kable(confMatTrain, caption="Train")
ktest <- kable(confMatTest, caption="Test")
kables(list(ktrain, ktest), format="html", caption="Confusion matrices")
```

# 3. Build your first CNN

Convolutional neural networks are a specific type of neural networks that use a reduced number of parameters compared
to conventional feed-forward neural network by exploiting the local structure that exists in the input data. These
networks are particularly performan on images which are highly structured data.

In this exercice you will be training a Convolutional neural network on the now famous [MNIST data set](
http://yann.lecun.com/exdb/mnist/).

## 3.1 Load the MNIST data

```{r load data mnist}
mnist   <- keras::dataset_mnist()
x_train <- mnist$train$x          # 3d array (60000,28,28)
y_train <- mnist$train$y          # 1d array (60000)
x_test  <- mnist$test$x           # 3d array (10000,28,28)
y_test  <- mnist$test$y           # 1d array (60000)

colorsDigit <- getColors(y_train, pal="Paired")
```

## 3.2 Visualize some examples

As usual, let's have a look at the data

```{r visualize mnist, fig.width=4, fig.height=4, fig.cap="36 Images from MNIST", fig.align="center"}
par(mfcol=c(6,6))
par(mar=c(0, 0, 0, 0), xaxs='i', yaxs='i')
for (idx in 1:36) { 
    im <- x_train[idx,,]
    im <- t(apply(im, 2, rev)) 
    image(1:28, 1:28, 1-im, ann=F, xaxt="n", yaxt="n", col=gray((0:255)/255), main=NULL)
}
```

For the needs of the model and the next step, the following chunk reshapes the data into matrices by flattening the
28x28 images into 756-long vectors and one-hot encoding the observation labels.

```{r prepare data mnist}
x_train <- keras::array_reshape(x_train, c(nrow(x_train), 784))
x_test  <- keras::array_reshape(x_test, c(nrow(x_test), 784))
x_train <- x_train/255
x_test  <- x_test/255
y_train_cat <- keras::to_categorical(y_train, 10)
y_test_cat  <- keras::to_categorical(y_test, 10)
```

To begin with, let's go for a naive strategy and run a PCA analysis on the input training samples to see how they
cluster.

**INCLASS WORK** Reuse the code you already saw in order to run a PCA on the MNIST data set and see how the digits
clusters in the first principal components.

```{r svd mnist}
resSVD <- svd(x_train, nu=2, nv=2)
# the scores are the principal components i.e the 
# low-dimensional representations of the samples
scores <- resSVD$u %*% diag(resSVD$d[1:2])
```

```{r plot PCA MNIST,fig.cap="PCA MNIST data ", fig.height=6, fig.width=6, fig.align="center"}
n=as.character(c(0,1,2,3,4,5,6,7,8,9))
colorsStudy <- getColors(as.character(c(0,1,2,3,4,5,6,7,8,9)),pal="Spectral")
ls_col=c()
for (v in colorsStudy){
  ls_col=c(ls_col,v)
}

# plot the two first principal components
plot(scores[,1],scores[,2],
     col=unlist(colorsStudy[y_train]),
     xlab=paste("dim1 = ",100*round(resSVD$d[1]^2/sum(resSVD$d^2),3),"%",sep = ""),
     ylab=paste("dim2 = ",100*round(resSVD$d[2]^2/sum(resSVD$d^2),3),"%",sep = ""),
     pch=16,main='MNIST representation')
legend("bottomright",legend=as.character(c(0,1,2,3,4,5,6,7,8,9)), pch=16, cex=0.8, col=ls_col)
```

**QUESTION 3)** Given the PCA representation of the MNIST dataset, do you think a linear model such as (mutliclass)
logistic regression can accurately classify the digits using the raw 756-long vector representation of the images? What
is, in your opinion, the advantage of a neural network over a logistic regression? Can you think of another model that
could be used to classify the MNIST dataset?

**ANSWER**:No we don't have a clear linear line that could separate the families. The avantage of NN is being able to overcome this non-linearity.
Another model that could be used to classify the MNIST dataset is a support vector machine (SVM) which is a type of supervised learning algorithm that can be used for classification or regression problems. 

&nbsp;


## 3.3 Build your "traditional model"

**INCLASS WORK** Build first a "traditional" neural network, compile and train it the MNIST data set.

```{r traditional model mnist, eval=F}
#### YOUR WORK HERE
model_mnist=keras_model_sequential()
model_mnist %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
  )

#model does not work with y_train_cat (dimension issues)so I did it with y_train
model_mnist %>% fit(x_train,y_train, epochs = 20,validation_split=0.1)

```

Let us now assess the performance of your model using the `evaluate` and `predict` functions.

```{r eval traditional model mnist, eval=F,echo=F}
#### get loss and accuracy on the test set
eval_test <- model_mnist %>% evaluate(
    x       = x_test,
    y       = y_test,
    verbose = 0
)
print(paste("Loss on test set:", round(eval_test["loss"], 5), sep=" "))
print(paste("Accuracy on test set:", round(eval_test["accuracy"], 5), sep=" "))

predictedTrain <- model_mnist %>% predict(x=x_train) %>% k_argmax()
predictedTest <- model_mnist %>% predict(x=x_test) %>% k_argmax() 

confMatTrain <- getConfusionMatrix(as.character(predictedTrain), as.character(y_train))
confMatTest <- getConfusionMatrix(as.character(predictedTest), as.character(y_test))

ktrain <- kable(confMatTrain, caption="Train")
ktest <- kable(confMatTest, caption="Test")
kables(list(ktrain, ktest), format="html", caption="Confusion matrices")
```

```{r prepare data mnist CNN}
mnist   <- keras::dataset_mnist()
x_train <- mnist$train$x          # 3d array (60000,28,28)
y_train <- mnist$train$y          # 1d array (60000)

x_test  <- mnist$test$x           # 3d array (10000,28,28)
y_test  <- mnist$test$y           # 1d array (60000)

x_train <- x_train/255
x_test  <- x_test/255

x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1)) # 4d array (60000, 28, 28, 1)
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1)) # 4d array (60000, 28, 28, 1)

y_train_cat <- keras::to_categorical(y_train, 10)
y_test_cat  <- keras::to_categorical(y_test, 10)
```
## 3.4 Build your CNN model

```{r cnn image, echo = F, out.width = '75%', fig.align="center"}
knitr::include_graphics('../../img/conv_2d-all.png')
```


**INCLASS WORK** Build the convolutional neural network displayed in the image above, compile and train it the MNIST
data set. Has your performance improved?


```{r cnn model mnist}
#### YOUR WORK HER
n1=4
n2=10

model2 <- keras_model_sequential()

model2 %>%
layer_conv_2d(filters = 32,kernel_size = c(5,5), input_shape = c(28,28,1), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64,kernel_size = c(5,5), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = "softmax")

model2 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))

y_train = to_categorical(y_train, num_classes = 10)

model2 %>% fit(x_train,y_train, epochs = 10,validation_split=0.2)
 

```


Eventually, here are the confusion matrices for our CNN trained on MNIST data.

```{r evaluate cnn model mnist, eval=T}
predictedTrain <- model2 %>% predict(x=x_train) %>% k_argmax()
predictedTest <- model2 %>% predict(x=x_test) %>% k_argmax()

confMatTrain <- getConfusionMatrix(as.character(predictedTrain), as.character(y_train))
confMatTest <- getConfusionMatrix(as.character(predictedTest), as.character(y_test))

ktrain <- kable(confMatTrain, caption="Train")
ktest <- kable(confMatTest, caption="Test")
kables(list(ktrain, ktest), format="html", caption="Confusion matrices")
```

`r if (knitr::is_html_output()) '# References {-}'`
